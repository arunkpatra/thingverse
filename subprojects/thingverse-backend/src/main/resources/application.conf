akka {
  loglevel = DEBUG
  log-config-on-start = off

  management {
    http {
      port = 8558
    }
  }

  http {
    server {
      max-connections = 1024
      preview {
        enable-http2 = on
      }
    }
  }
  actor {
    provider = cluster

    serialization-bindings {
      "com.thingverse.api.serialization.CborSerializable" = jackson-cbor
    }
  }

  # For the sample, just bind to loopback and do not allow access from the network
  # the port is overridden by the logic in main class
  remote.artery {
    canonical.port = 0
    //    canonical.hostname = 127.0.0.1
    bind.bind-timeout = 10s
  }

  extensions = ["akka.cluster.metrics.ClusterMetricsExtension"]

  cluster {
    downing-provider-class = "com.thingverse.backend.downing.ThingverseDowningProvider"
    //    seed-nodes = ["akka://thingverse-backend@127.0.0.1:2551", "akka://thingverse-backend@127.0.0.1:2552"]
    seed-nodes = ["akka://thingverse-backend@127.0.0.1:2551"]
    //    seed-nodes = []
    //roles = ["write-model", "read-model"]

    role {
      read-model {
        min-nr-of-members = 1
      }
      write-model {
        min-nr-of-members = 1
      }
    }
    sharding {
      remember-entities = off
      # use value like 120s if you want to passivate
      passivate-idle-entity-after = off
    }
    metrics {
      collector {
        # Enable or disable metrics collector for load-balancing nodes.
        # Metrics collection can also be controlled at runtime by sending control messages
        # to /system/cluster-metrics actor: `akka.cluster.metrics.{CollectionStartMessage,CollectionStopMessage}`
        enabled = off
        # FQCN of the metrics collector implementation.
        # It must implement `akka.cluster.metrics.MetricsCollector` and
        # have public constructor with akka.actor.ActorSystem parameter.
        # Will try to load in the following order of priority:
        # 1) configured custom collector 2) internal `SigarMetricsCollector` 3) internal `JmxMetricsCollector`
        provider = "com.thingverse.backend.metrics.collector.ThingverseMetricsCollector"
        # Try all 3 available collector providers, or else fail on the configured custom collector provider.
        fallback = true
        # How often metrics are sampled on a node.
        # Shorter interval will collect the metrics more often.
        # Also controls frequency of the metrics publication to the node system event bus.
        sample-interval = 3s
        # How often a node publishes metrics information to the other nodes in the cluster.
        # Shorter interval will publish the metrics gossip more often.
        gossip-interval = 3s
        # How quickly the exponential weighting of past data is decayed compared to
        # new data. Set lower to increase the bias toward newer values.
        # The relevance of each data sample is halved for every passing half-life
        # duration, i.e. after 4 times the half-life, a data sample’s relevance is
        # reduced to 6% of its original relevance. The initial relevance of a data
        # sample is given by 1 – 0.5 ^ (collect-interval / half-life).
        # See http://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average
        moving-average-half-life = 12s
      }
    }
  }

  # use Cassandra to store both snapshots and the events of the persistent backend
  # See https://doc.akka.io/docs/akka-persistence-cassandra/current/configuration.html#contact-points-configuration
  persistence {
    journal {
      plugin = "akka.persistence.cassandra.journal"
      auto-start-journals = ["akka.persistence.cassandra.journal"]
    }
    snapshot-store {
      plugin = "akka.persistence.cassandra.snapshot"
      auto-start-snapshot-stores = ["akka.persistence.cassandra.snapshot"]
    }
    cassandra {
      events-by-tag {
        bucket-size = "Day"
        # for reduced latency
        eventual-consistency-delay = 2s
        flush-interval = 250ms
        pubsub-notification = on
        first-time-bucket = "20200115T00:00"
      }
      journal {
        tables-autocreate = on
        keyspace-autocreate = on
      }
      query {
        refresh-interval = 2s
      }
      snapshot {
        keyspace-autocreate = on
        tables-autocreate = on
      }
    }
  }
}
datastax-java-driver {
  basic {
    request {
      timeout = 15 seconds
    }
    contact-points = ["127.0.0.1:9043"]
    load-balancing-policy.local-datacenter = datacenter1
  }
  advanced {
    timestamp-generator {
      force-java-clock = true
    }
    reconnect-on-init = on
  }
}

event-processor {
  id = "EventProcessor"            // type name of sharded event processor
  keep-alive-interval = 2 seconds  // event-processors ping interval
  tag-prefix = "tag"               // event-processor tag prefix
  parallelism = 4                  // number of event processors
}

thingverse {
  things {
    remote-thing {
      snapshot-after-events = 1000
      max-snapshots = 2
    }
  }
}

thingverse-akka-downing {
  # Time margin after which shards or singletons that belonged to a downed/removed
  #  partition are created in surviving partition. The purpose of this margin is that
  #  in case of a network partition the persistent backend in the non-surviving partitions
  #  must be stopped before corresponding persistent backend are started somewhere else.
  #
  # This margin should be picked based on cluster size, a larger number of nodes introducing
  #  more variance in timing.
  #
  # Disable with "off" or specify a duration to enable.
  #
  # See akka.cluster.down-removal-margin
  down-removal-margin = 5s

  # Time margin after which unreachable nodes in a stable cluster state (i.e. no nodes changed
  #  their membership state or their reachability) are treated as permanently unreachable, and
  #  the split-brain resolution strategy kicks in.
  stable-after = 2s

  # The active strategy is one of static-quorum, keep-majority and keep-oldest. It is triggered
  #  after the cluster configuration has been stable for an interval of 'stable-after'.
  #
  # static-quorum defines a fixed number of nodes, and a network partition must have at least
  #  this number of reachable nodes (in a given role, if that is specified) in order to be allowed
  #  to survive. If the quorum size is picked bigger than half the maximum number of cluster nodes,
  #  this strategy is completely robust. It does not however work well with a dynamically growing
  #  (or shrinking) cluster.
  #
  # keep-majority uses the number of cluster nodes as the baseline and requires a network partition
  #  to have more than half that number of (reachable) nodes in order to be allowed to survive. This
  #  fully supports elastically growing and shrinking clusters, but there are rare race conditions
  #  that can lead to both partitions to be downed or - potentially worse - both partitions to survive.
  #
  # keep-oldest requires the oldest member to be part of a partition for it to survive. This can be
  #  useful since the oldest node is where cluster singletons are running, so this strategy does not
  #  singletons to be migrated and restarted. It reliably prevents split brain, but it can lead to
  #  a situation where 2 nodes survive and 25 nodes are downed. To deal with the pathological special
  #  case that the oldest node is in a network partition of its own, the flag 'down-if-alone' can be
  #  used to specify the oldest node if it is all by itself.
  active-strategy = role-based-static-quorum

  # If initial cluster startup delay causes network partitions to be detected, setting
  #  akka.cluster.min-nr-or-members to the value of quorum-size can delay checks
  static-quorum {
    # minimum number of nodes that the cluster must have
    quorum-size = undefined

    # if the 'role' is defined the decision is based only on members with that 'role'
    role = ""
  }

  keep-majority {
    role = ""
  }

  keep-oldest {
    # if on, activates special treatment for the situation that the oldest node is the only node to
    #  be split off, causing it to be downed and the rest of the cluster to survive.
    down-if-alone = on
  }

  role-based-static-quorum = [
    {
      quorum-size = 1
      # if the 'role' is defined the decision is based only on members with that 'role'
      role = "read-model"
    },
    {
      quorum-size = 1
      # if the 'role' is defined the decision is based only on members with that 'role'
      role = "write-model"
    }
  ]
}